Right now I have an idea, it's very vague but I think it might turn into something more substantial over time.

I've been thinking about filtering online trolls.  With online communities right now there is a large problem with harassment and false information.  Bad actors dont suffer any real consequences, banning them puts a stop to their activities but they are free to go about their lives or simply try again somewhere else.  We get angry at facebook and twitter for not clamping down on misinformation but the tolls they have wouldn't work even if they were trying their darndest.  Unless facebook is willing to hire as many moderators as Russia, China and every company with an astroturfing operation are willing to hire paid advocates they can't filter the internet.  The only thing that would work is a machine learning process that can objectively identify the behavior you want to filter out.  However this just starts a race, can you make a machine learning process to beat the filter faster then the filter can be improved?  This shouldn't be a race we need to win in order to get news and have conversations online.

The crux of the idea is this: It is potentially far more objective to find what the harassers are trying to stop then to find the harassment.  Let me give you a metaphor.  Suppose you are trying to listen to a woman who can only speak in a whisper.  There is a man who doesn't want her to speak so he is shouting to make sure you can't hear here.  Would it help you and her more if I gave you an accurate written transcript of what is being shouted at her or an accurate written transcript of what she is whispering?  Suppose you wanted to stop the shouting.  Would it help you more to have the womans words or would it help you more to take certain words and forbid anyone from saying those words?

Can we figure out what views people are trying to silence?  I dont think this is an easy task but I think it is a lot more viable then identifying all harassment.  The biggest reason is that you are looking for someone who wants to be found instead of someone who doesn't want you to find them.  Suppose that you could hear the whispering woman if she repeats herself 10 times, she will do that.  Suppose you could learn to filter out the shouting man if he repeats himself 10 times.  He's going to keep changing things up or he might tag team with a friend.  This is important with machine learning because machine learning improves with repeated data.

Of course this is only half the problem.  Once you have identified who is being harassed you still need to solve the problem.  However I think that just identifying the problem would do an enormous amount to solve it.  Newspaper editors for instance do an excellent job selecting a wide range of views which are interesting to the sort of person who becomes a newspaper editor.  The Reddit "upvote/downvote" system is very prone to takes the process of selecting biased views and crowdsources it.  If those newspaper editors could have a numerical metric that says "this is the sort of view your newspaper ignores" I think they would make far better cohices then their own fuzzy self assessment.  If the average redditor was told "you just upvoted something that is frequently used to drown out other views" I think that would give most humans pause.  It could break the cycle of downvote upvote wars fuelled by the fear that the other side will fight without opposition.  If there are fewer people chosing to drown out other forces, that makes it harder for the bad actors who dont care.
