My class capstone presentations were yesterday so a dozen classmates and myself presented the research we had been doing.  Our data science "bootcamp" has been keeping us constantly racing to meet deadline after deadline so research has primarily been done in the past week even mostly in just a few days.  Because there were half a dozen projects being presented in a short amount of time, you have a phrase that is obviously going to come up several times...

"Ideally we would want to gather more data about..."

or alternatively

"If I had more time I would look into..."

And to be clear, I totally count myself among the offenders here.  [My project](https://johnfwhitesell.github.io/Boring/) was on commuter transit choices.  When the 2012-2017 American Community Survey data on commuter transit flows get's released it will be easy for me to answer questions that are difficult or impossible to answer now.  This is an inevitable part of research, there will always be more information coming out.  It's extremely encourage if you stop to think about it.  As great as the future is, I worry sometimes that scientists at all levels (from PhD holders to grade schoolers) can fall into the trap of thinking about the perfect dataset too much.

There are good reasons we think about a perfect dataset, it's an important part of teaching people to think like scientists (particularly when it comes to statistics).  Anyone who has ever rooted for a sports team sooner or later falls into the trap of "well everyone I know thinks this player is amazing so they they must be great".  You want any scientist to be able to spot the problem in the data behind that reasoning in a nanosecond.  Training people to avoid bad analysis is mostly about teaching them to come up with objective metrics.  Instead of asking my friends how good this player is, I will look at the records of play, that's more objective.  Not all statistics are created equal so I will realize that slugging percentage isn't as important as on base percentage, that's more objective again.  Start looking at the other factors that go into performance and eventually you get all the way to wins over replacement and moneyball.  While we aren't all going to invent sabremetrics, a big part of academics is learning more and more sophisticated ways to be objective.  We learn about fundemental ideas like sampling bias (does this really show what we want?), then we build that into metrics like standard deviation (how important is that difference really?) and eventually we start considering more and more complex intellectual traps (Is this model high variance?  Try cross validation.  Am I p-hacking?  Well better use a holdout.)  Objectivity is a great goal and we should always keep it in mind!  But it isn't enough by itself.

Consider this: computers have been more objective then humans for centuries but right now the very high of purely computer based analysis is very crude classification.  "Unsupervised learning" is a process by which a computer takes a sample and divides it into groups.  That's it.  That's all a computer can do.  The computer can tell you what makes those groups different but it cant tell you why.  It can't tell you if those differences are useful as predictors or as ways to change the groups.  These groupings are excellent as a preliminary step.  For example my classmate Mike used unsupervised learning groupings to make a [classification program](https://github.com/mjschillawski/capstone_project).  However by themselves they are useless, we dont want our scientists to be computers, we want something useful.  So while it's great to think "what's the perfect dataset here" dont lose sight of the dataset that we have.

This is particularly important for anyone who is comming out of a data science bootcamp.  The tools you have are the best tools in the world.  Most data analysis being done is just being done with regression analysis anyways.  We have more then that, we have our random forest models for decision trees, neural networks for classification, data normalization and transformation and large dataset tools like databricks and amazon web services.  These are the tools that are building every model in the world.  So if a task is humanly possible, you should know that it is possible for you, the data science bootcamp graduate.  Experience and resources matter but so does persistance.  So remember that before you say "I want more data" or "More research could look into XYZ".  Sometimes there is a proprietary dataset or a more powerful computer that is important but most important findings are being done with nothing but tools that are in the arsenal of every single one of us.

538 had a [great article](https://fivethirtyeight.com/features/the-pay-gap-is-way-too-entrenched-to-be-solved-by-women-alone/) which discussed a major problem with studying the gender wage gap.  Discrimination isn't the only thing which makes a field high or low compensation but there still are differences between fields which aren't driven by discrimination.  Many people look at this imperfect data and just give up, they only look at pay differences in the same job title.  But some researchers in Minnesota didn't settle for just giving up, they came up with four metrics to compare different jobs.  Our scientific training tells us to be worried, it's not a perfectly objective process.  But dont let your training be a bias towards inaction, for instance did you also immediately think that by pooling the data in this way they also reduced spurious correlations?  Did you think of that as quickly as your instinct to worry about how objective their feature engineering was?  I know I didn't.  It's always good to have more and better data but dont forget what you can do with elbow grease and feature engineering.
